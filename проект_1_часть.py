# -*- coding: utf-8 -*-
"""проект 1 часть.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TT-2PnEWZVq-SYljasfB-7FuSbVNbK-r
"""

from google.colab import drive, files
drive.mount('/content/drive')
uploaded = files.upload()
for fn in uploaded.keys():
    if fn.endswith(".json"):
        json_name = fn
!mv "$json_name" kaggle.json
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d meowmeowmeowmeowmeow/gtsrb-german-traffic-sign -p /content
!unzip -o /content/gtsrb-german-traffic-sign.zip -d /content/gtsrb

import os
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split, Subset

train_root = "/content/gtsrb/Train"

# Загружаем необработанный датасет без трансформаций
dataset_raw = datasets.ImageFolder(root=train_root, transform=None)

# Считаем количество изображений по каждому классу
class_counts = {}
for _, label in dataset_raw.samples:
    class_counts[label] = class_counts.get(label, 0) + 1

class_counts_sorted = dict(sorted(class_counts.items(), key=lambda x: x[0]))

# Визуализация распределения
plt.figure(figsize=(15,5))
plt.bar(class_counts_sorted.keys(), class_counts_sorted.values())
plt.title("Распределение изображений по классам")
plt.xlabel("Класс")
plt.ylabel("Количество изображений")
plt.show()

len(dataset_raw), len(class_counts_sorted)

fig, axes = plt.subplots(2, 5, figsize=(12, 6))
indices = np.random.choice(range(len(dataset_raw)), 10, replace=False)

for ax, idx in zip(axes.flatten(), indices):
    img, label = dataset_raw[idx]
    ax.imshow(img)
    ax.set_title(f"Class: {label}")
    ax.axis('off')

plt.tight_layout()
plt.show()

from torchvision import transforms

# Трансформации
train_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomAffine(degrees=5, translate=(0.02, 0.02), scale=(0.95, 1.05)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])

val_test_transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))
])

# Разбиение
total = len(dataset_raw)
train_size = int(total * 0.70)
val_size   = int(total * 0.15)
test_size  = total - train_size - val_size

generator = torch.Generator().manual_seed(42)
splits = random_split(dataset_raw, [train_size, val_size, test_size], generator=generator)

train_indices = splits[0].indices
val_indices   = splits[1].indices
test_indices  = splits[2].indices

# Создаём датасеты с трансформациями
dataset_train = datasets.ImageFolder(root=train_root, transform=train_transform)
dataset_val   = datasets.ImageFolder(root=train_root, transform=val_test_transform)
dataset_test  = datasets.ImageFolder(root=train_root, transform=val_test_transform)

train_dataset = Subset(dataset_train, train_indices)
val_dataset   = Subset(dataset_val, val_indices)
test_dataset  = Subset(dataset_test, test_indices)

# DataLoaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

len(train_dataset), len(val_dataset), len(test_dataset)

import os, shutil

original_test_dir = "/content/gtsrb/Test"
processed_test_dir = "/content/gtsrb/ProcessedTest"
os.makedirs(processed_test_dir, exist_ok=True)
df = pd.read_csv("/content/gtsrb/Test.csv", sep=',')
for class_id in range(43):
    os.makedirs(os.path.join(processed_test_dir, f"{class_id:02d}"), exist_ok=True)
copied = 0
missing = 0

for _, row in df.iterrows():
    filename = row["Path"].split("/")[-1]
    class_id = int(row["ClassId"])

    src = os.path.join(original_test_dir, filename)
    dst = os.path.join(processed_test_dir, f"{class_id:02d}", filename)

    if os.path.exists(src):
        if not os.path.exists(dst):
            shutil.copy(src, dst)
            copied += 1
    else:
        missing += 1

final_test_dataset = datasets.ImageFolder(
    root=processed_test_dir,
    transform=val_test_transform
)

test_loader_final = DataLoader(final_test_dataset, batch_size=64, shuffle=False)

print("Final test images:", len(final_test_dataset))
print("Number of classes:", len(final_test_dataset.classes))

import json
import os

meta_dir = "/content/gtsrb_split_meta"
os.makedirs(meta_dir, exist_ok=True)

with open(os.path.join(meta_dir, "train_indices.json"), "w") as f:
    json.dump(train_indices, f)

with open(os.path.join(meta_dir, "val_indices.json"), "w") as f:
    json.dump(val_indices, f)

with open(os.path.join(meta_dir, "test_indices.json"), "w") as f:
    json.dump(test_indices, f)

with open(os.path.join(meta_dir, "class_to_idx.json"), "w") as f:
    json.dump(dataset_train.class_to_idx, f)

print("Meta saved in:", meta_dir)

import shutil
import os

TARGET_DIR = "/content/drive/MyDrive/GTSRB_prepared"
if os.path.exists(TARGET_DIR):
    shutil.rmtree(TARGET_DIR)
os.makedirs(TARGET_DIR, exist_ok=True)
src_train = "/content/gtsrb/Train"
dst_train = os.path.join(TARGET_DIR, "Train")
shutil.copytree(src_train, dst_train)
src_test_raw = "/content/gtsrb/Test"
dst_test_raw = os.path.join(TARGET_DIR, "Test_raw")
shutil.copytree(src_test_raw, dst_test_raw)
src_processed_test = "/content/gtsrb/ProcessedTest"
dst_processed_test = os.path.join(TARGET_DIR, "ProcessedTest")

if os.path.exists(src_processed_test):
    shutil.copytree(src_processed_test, dst_processed_test)
else:
    print("ProcessedTest не найден — пропускаю копирование.")
src_meta = "/content/gtsrb_split_meta"
dst_meta = os.path.join(TARGET_DIR, "split_meta")
shutil.copytree(src_meta, dst_meta)

print("Готовые данные успешно перенесены в Google Drive:")
print(TARGET_DIR)

!ls "/content/drive/MyDrive/GTSRB_prepared"

"""С помощью этой ячейки можно скачать пердобработанные файлы"""

SOURCE_DIR = "/content/drive/MyDrive/GTSRB_prepared"
TARGET_DIR = "/content/GTSRB_prepared"
ZIP_PATH = "/content/GTSRB_prepared.zip"
print("\n Проверка положения данных...")
if not os.path.exists(SOURCE_DIR):
    raise FileNotFoundError(
        "Папка GTSRB_prepared не найдена на Google Drive.\n"
        "Убедитесь, что у вас есть доступ к общей папке."
    )
print("Найдено.")
print("\nКопирование данных в локальную среду Colab")

if os.path.exists(TARGET_DIR):
    shutil.rmtree(TARGET_DIR)

shutil.copytree(SOURCE_DIR, TARGET_DIR)
print("Данные успешно скопированы.")
print("\n Создание ZIP архива...")
if os.path.exists(ZIP_PATH):
    os.remove(ZIP_PATH)

shutil.make_archive("/content/GTSRB_prepared", 'zip', TARGET_DIR)
print(f"ZIP архив создан: {ZIP_PATH}")

print("\nСкачивание архива.")
files.download(ZIP_PATH)

"""# **Тема (ViT)**"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt
import numpy as np
import json
import os
import time
from tqdm import tqdm
from datetime import datetime
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

IMAGE_SIZE = 64
PATCH_SIZE = 8
BATCH_SIZE = 128
NUM_EPOCHS = 30
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Устройство: {DEVICE}")
print(f"Конфигурация: {IMAGE_SIZE}x{IMAGE_SIZE}, Batch={BATCH_SIZE}")

def load_data():
    """Загрузка предобработанных данных"""

    DATA_PATH = "/content/GTSRB_prepared"
    META_PATH = os.path.join(DATA_PATH, "split_meta")
    TRAIN_PATH = os.path.join(DATA_PATH, "Train")
    PROCESSED_TEST_PATH = os.path.join(DATA_PATH, "ProcessedTest")

    def load_json(filepath):
        with open(filepath, 'r') as f:
            return json.load(f)

    train_indices = load_json(os.path.join(META_PATH, 'train_indices.json'))
    val_indices = load_json(os.path.join(META_PATH, 'val_indices.json'))
    test_indices = load_json(os.path.join(META_PATH, 'test_indices.json'))
    class_to_idx = load_json(os.path.join(META_PATH, 'class_to_idx.json'))

    class_names = [str(k) for k in sorted(class_to_idx.keys(), key=lambda x: int(x))]

    print(f"Загружено: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}")
    print(f"Классов: {len(class_names)}")

    return (DATA_PATH, TRAIN_PATH, PROCESSED_TEST_PATH,
            train_indices, val_indices, test_indices, class_names)

(DATA_PATH, TRAIN_PATH, PROCESSED_TEST_PATH,
 train_indices, val_indices, test_indices, class_names) = load_data()

train_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.RandomRotation(10),
    transforms.RandomHorizontalFlip(p=0.3),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

val_test_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

def create_dataloaders():
    """Создание DataLoader'ов"""

    train_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=train_transform)
    val_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=val_test_transform)
    test_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=val_test_transform)

    train_subset = Subset(train_dataset, train_indices)
    val_subset = Subset(val_dataset, val_indices)
    test_subset = Subset(test_dataset, test_indices)

    train_loader = DataLoader(
        train_subset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_subset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_subset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    final_test_dataset = datasets.ImageFolder(
        root=PROCESSED_TEST_PATH,
        transform=val_test_transform
    )

    final_test_loader = DataLoader(
        final_test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    print(f"Batch size: {BATCH_SIZE}")
    print(f"Батчей: Train={len(train_loader)}, Val={len(val_loader)}")

    return train_loader, val_loader, test_loader, final_test_loader

train_loader, val_loader, test_loader, final_test_loader = create_dataloaders()

class SimplePatchEmbedding(nn.Module):
    """Упрощенный патч эмбеддинг"""
    def __init__(self, img_size=64, patch_size=8, in_chans=3, embed_dim=192):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = img_size // patch_size
        self.num_patches = self.grid_size ** 2

        self.proj = nn.Conv2d(in_chans, embed_dim,
                             kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x)
        x = x.flatten(2)
        x = x.transpose(1, 2)
        return x

class SimpleViT(nn.Module):
    """ ViT для быстрого обучения"""

    def __init__(self, img_size=64, patch_size=8, in_chans=3, num_classes=43,
                 embed_dim=192, depth=4, num_heads=3, mlp_ratio=2., dropout=0.1):
        super().__init__()

        self.img_size = img_size
        self.patch_size = patch_size
        self.num_classes = num_classes

        self.patch_embed = SimplePatchEmbedding(img_size, patch_size, in_chans, embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(dropout)

        self.blocks = nn.Sequential(*[
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=int(embed_dim * mlp_ratio),
                dropout=dropout,
                activation='relu',
                batch_first=True,
                norm_first=True
            ) for _ in range(depth)
        ])

        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, num_classes)
        )

        self._init_weights()

    def _init_weights(self):
        """Простая инициализация"""
        nn.init.normal_(self.cls_token, std=0.02)
        nn.init.normal_(self.pos_embed, std=0.02)

        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        x = self.patch_embed(x)

        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)

        x = x + self.pos_embed
        x = self.pos_drop(x)

        x = self.blocks(x)

        x = self.norm(x)
        x = x[:, 0]
        x = self.head(x)

        return x

def create_simple_vit(num_classes=43):
    """Создание оптимизированной ViT"""
    model = SimpleViT(
        img_size=IMAGE_SIZE,
        patch_size=PATCH_SIZE,
        num_classes=num_classes,
        embed_dim=192,
        depth=4,
        num_heads=3,
        mlp_ratio=2.0,
        dropout=0.1
    )
    return model

model = create_simple_vit(num_classes=len(class_names))
model = model.to(DEVICE)
print(f"Создана модель: {sum(p.numel() for p in model.parameters()):,} параметров")

def train_epoch_fast(model, loader, criterion, optimizer, scaler):
    """Эпоха обучения"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc="Training", leave=False)

    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(DEVICE), labels.to(DEVICE)

        with autocast():
            outputs = model(images)
            loss = criterion(outputs, labels)

        optimizer.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        if batch_idx % 10 == 0:
            pbar.set_postfix({
                'Loss': f'{total_loss/(batch_idx+1):.3f}',
                'Acc': f'{100.*correct/total:.1f}%'
            })

    epoch_loss = total_loss / len(loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

def validate_epoch_fast(model, loader, criterion):
    """Быстрая валидация"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in tqdm(loader, desc="Validation", leave=False):
            images, labels = images.to(DEVICE), labels.to(DEVICE)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = total_loss / len(loader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc

def train_model(model, train_loader, val_loader, num_epochs=30):
    """Основной цикл обучения"""
    print(f"\nНАЧИНАЕМ ОБУЧЕНИЕ")
    print(f"Ожидаемое время: ~{num_epochs*2} минут")

    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE * 10,
        epochs=num_epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.3
    )
    scaler = GradScaler()

    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [],
        'lr': []
    }

    best_acc = 0
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"\n{'='*50}")
        print(f"ЭПОХА {epoch+1}/{num_epochs}")
        print(f"{'='*50}")

        train_loss, train_acc = train_epoch_fast(
            model, train_loader, criterion, optimizer, scaler
        )

        val_loss, val_acc = validate_epoch_fast(model, val_loader, criterion)

        scheduler.step()

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['lr'].append(scheduler.get_last_lr()[0])

        print(f"Train: Loss={train_loss:.3f}, Acc={train_acc:.2f}%")
        print(f"Val:   Loss={val_loss:.3f}, Acc={val_acc:.2f}%")
        print(f"LR:    {scheduler.get_last_lr()[0]:.6f}")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'train_acc': train_acc,
            }, 'best_vit_model.pth')
            print(f"Сохранена лучшая модель! Acc: {val_acc:.2f}%")

        if epoch > 0 and epoch % 5 == 0:
            elapsed = time.time() - start_time
            remaining = elapsed / (epoch + 1) * (num_epochs - epoch - 1)
            print(f"⏱Прошло: {elapsed/60:.1f} мин, Осталось: {remaining/60:.1f} мин")

    total_time = time.time() - start_time
    print(f"\nОБУЧЕНИЕ ЗАВЕРШЕНО!")
    print(f"Лучшая точность: {best_acc:.2f}%")
    print(f"Общее время: {total_time/60:.1f} минут")

    # Загружаем лучшую модель
    checkpoint = torch.load('best_vit_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])

    return model, history

if torch.cuda.is_available():
  torch.cuda.empty_cache()

print("\n" + "="*70)
print("ViT ДЛЯ КЛАССИФИКАЦИИ ДОРОЖНЫХ ЗНАКОВ")
print("="*70)

print("\nПРОВЕРКА ДАННЫХ...")
sample_batch = next(iter(train_loader))
images, labels = sample_batch
print(f"Размер батча: {images.shape}")
print(f"Диапазон значений: [{images.min():.3f}, {images.max():.3f}]")

# print("\nОБУЧЕНИЕ МОДЕЛИ...")
# trained_model, history = train_model(
#         model, train_loader, val_loader, num_epochs=NUM_EPOCHS
# )

import os

if os.path.exists('best_vit_model.pth'):
  print("\nЗагружаю модель...")
  checkpoint = torch.load('best_vit_model.pth', map_location=DEVICE)
  model.load_state_dict(checkpoint['model_state_dict'])
  print(f"Модель загружена с точностью {checkpoint['val_acc']:.2f}%")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

def load_trained_model(model_path='best_vit_model.pth'):
    """Загрузка обученной модели"""

    model = SimpleViT(
        img_size=IMAGE_SIZE,
        patch_size=PATCH_SIZE,
        num_classes=43,
        embed_dim=192,
        depth=4,
        num_heads=3,
        mlp_ratio=2.0,
        dropout=0.1
    )

    checkpoint = torch.load(model_path, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(DEVICE)
    model.eval()

    print(f"Модель загружена из {model_path}")
    print(f"Точность при обучении: {checkpoint['val_acc']:.2f}%")
    print(f"Эпоха сохранения: {checkpoint['epoch'] + 1}")

    return model, checkpoint

def test_model_comprehensive(model, test_loader, class_names, name="Test"):
    """Комплексное тестирование модели"""
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    incorrect_samples = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc=f"Testing {name}"):
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

            incorrect_mask = predicted != labels
            if incorrect_mask.any():
                for i in range(len(images)):
                    if incorrect_mask[i]:
                        incorrect_samples.append({
                            'image': images[i].cpu(),
                            'true_label': labels[i].item(),
                            'pred_label': predicted[i].item(),
                            'prob': probs[i][predicted[i]].item()
                        })

    accuracy = 100.0 * (np.array(all_preds) == np.array(all_labels)).sum() / len(all_preds)

    print(f"\n{'='*60}")
    print(f"РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ: {name}")
    print(f"{'='*60}")
    print(f"Общая точность: {accuracy:.2f}%")
    print(f"Количество примеров: {len(all_preds)}")
    print(f"Ошибок: {len(incorrect_samples)}")

    print(f"\nCLASSIFICATION REPORT:")
    report = classification_report(
        all_labels, all_preds,
        target_names=[str(i) for i in range(len(class_names))],
        digits=3,
        output_dict=False
    )
    print(report)


    if incorrect_samples:
        print(f"\nАНАЛИЗ ОШИБОК (первые 5):")
        for i, sample in enumerate(incorrect_samples[:5]):
            print(f"  Пример {i+1}: True={sample['true_label']}, "
                  f"Pred={sample['pred_label']}, "
                  f"Confidence={sample['prob']:.3f}")


    return accuracy, all_preds, all_labels, incorrect_samples

def test_on_all_datasets(model):
    """Тестирование модели на всех доступных датасетах"""

    DATA_PATH = "/content/GTSRB_prepared"
    TRAIN_PATH = os.path.join(DATA_PATH, "Train")
    PROCESSED_TEST_PATH = os.path.join(DATA_PATH, "ProcessedTest")

    test_transform = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    print("\n" + "="*70)
    print("ТЕСТИРОВАНИЕ НА ВНУТРЕННЕМ ТЕСТЕ")
    print("="*70)

    test_dataset = datasets.ImageFolder(root=TRAIN_PATH, transform=test_transform)

    import json
    META_PATH = os.path.join(DATA_PATH, "split_meta")
    with open(os.path.join(META_PATH, 'test_indices.json'), 'r') as f:
        test_indices = json.load(f)
    with open(os.path.join(META_PATH, 'class_to_idx.json'), 'r') as f:
        class_to_idx = json.load(f)

    class_names = [str(k) for k in sorted(class_to_idx.keys(), key=lambda x: int(x))]
    test_subset = Subset(test_dataset, test_indices)
    test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    test_accuracy, _, _, _ = test_model_comprehensive(
        model, test_loader, class_names, "Test Set"
    )

    print("\n" + "="*70)
    print("ТЕСТИРОВАНИЕ НА ВАЛИДАЦИОННОМ НАБОРЕ")
    print("="*70)

    with open(os.path.join(META_PATH, 'val_indices.json'), 'r') as f:
        val_indices = json.load(f)
    val_subset = Subset(test_dataset, val_indices)
    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    val_accuracy, _, _, _ = test_model_comprehensive(
        model, val_loader, class_names, "Validation Set"
    )

    print("\n" + "="*70)
    print("ТЕСТИРОВАНИЕ НА ОБУЧАЮЩЕМ НАБОРЕ (выборка)")
    print("="*70)

    with open(os.path.join(META_PATH, 'train_indices.json'), 'r') as f:
        train_indices = json.load(f)

    sample_indices = train_indices[:1000]
    train_subset = Subset(test_dataset, sample_indices)
    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    train_accuracy, _, _, _ = test_model_comprehensive(
        model, train_loader, class_names, "Train Set (Sample)"
    )

    print("\n" + "="*70)
    print("ИТОГОВЫЙ ОТЧЕТ ПО МОДЕЛИ")
    print("="*70)
    print(f"Test Set Accuracy:        {test_accuracy:.2f}%")
    print(f"Validation Set Accuracy:  {val_accuracy:.2f}%")
    print(f"Train Set Accuracy:       {train_accuracy:.2f}%")
    print("\n" + "="*70)

    return {
        'test_accuracy': test_accuracy,
        'val_accuracy': val_accuracy,
        'train_accuracy': train_accuracy
    }

def main_testing():
    """Основная функция для тестирования модели"""
    print("\n" + "="*70)
    print("ТЕСТИРОВАНИЕ СОХРАНЕННОЙ МОДЕЛИ ViT")
    print("="*70)

    print("\nЗАГРУЗКА МОДЕЛИ...")
    model, checkpoint = load_trained_model('best_vit_model.pth')

    print("\nТЕСТИРОВАНИЕ...")
    results = test_on_all_datasets(model)

    print("\nСОХРАНЕНИЕ РЕЗУЛЬТАТОВ...")
    import pandas as pd
    results_df = pd.DataFrame([results])
    results_df.to_csv('model_test_results.csv', index=False)
    print("Результаты сохранены в 'model_test_results.csv'")

    return model, results

if torch.cuda.is_available():
  torch.cuda.empty_cache()

trained_model, results = main_testing()